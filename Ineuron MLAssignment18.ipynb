{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91949635",
   "metadata": {},
   "source": [
    "1. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e52742",
   "metadata": {},
   "source": [
    "Some applications of unsupervised learning include natural language processing, image and video analysis, anomaly detection, customer segmentation, and recommendation engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb7338",
   "metadata": {},
   "source": [
    "2. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a3e57",
   "metadata": {},
   "source": [
    "Density-Based Clustering\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "OPTICS (Ordering Points to Identify Clustering Structure)\n",
    "HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)\n",
    "Hierarchical Clustering\n",
    "Fuzzy Clustering\n",
    "Partitioning Clustering\n",
    "PAM (Partitioning Around Medoids)\n",
    "Grid-Based Clustering\n",
    "\n",
    "Density-Based Clustering\n",
    "In this method, the clusters are created based upon the density of the data points which are represented in the data space. The regions that become dense due to the huge number of data points residing in that region are considered as clusters.\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "DBSCAN groups data points together based on the distance metric. It follows the criterion for a minimum number of data points. It can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers.It takes two parameters – eps and minimum points. Eps indicates how close the data points should be to be considered as neighbors. The criterion for minimum points should be completed to consider that region as a dense region.\n",
    "\n",
    "OPTICS (Ordering Points to Identify Clustering Structure)\n",
    "OPTICS follows a similar process as DBSCAN but overcomes one of its drawbacks, i.e. inability to form clusters from data of arbitrary density. It considers two more parameters which are core distance and reachability distance. Core distance indicates whether the data point being considered is core or not by setting a minimum value for it.\n",
    "\n",
    "Reachability distance is the maximum of core distance and the value of distance metric that is used for calculating the distance among two data points. One thing to consider about reachability distance is that its value remains not defined if one of the data points is a core point.\n",
    "\n",
    "HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)\n",
    "HDBSCAN is a density-based clustering method that extends the DBSCAN methodology by converting it to a hierarchical clustering algorithm.\n",
    "\n",
    "Hierarchical Clustering\n",
    "Hierarchical Clustering groups (Agglomerative or also called as Bottom-Up Approach) or divides (Divisive or also called as Top-Down Approach) the clusters based on the distance metrics.\n",
    "\n",
    "In agglomerative clustering, initially, each data point acts as a cluster, and then it groups the clusters one by one. This comes under in one of the most sought-after clustering methods.\n",
    "\n",
    "Divisive is the opposite of Agglomerative, it starts off with all the points into one cluster and divides them to create more clusters. These algorithms create a distance matrix of all the existing clusters and perform the linkage between the clusters depending on the criteria of the linkage. The clustering of the data points is represented by using a dendrogram. There are different types of linkages: –\n",
    "\n",
    "o    Single Linkage: – In single linkage the distance between the two clusters is the shortest distance between points in those two clusters.\n",
    "\n",
    "o   Complete Linkage: – In complete linkage, the distance between the two clusters is the farthest distance between points in those two clusters.\n",
    "\n",
    "o   Average Linkage: – In average linkage the distance between the two clusters is the average distance of every point in the cluster with every point in another cluster.\n",
    "\n",
    "Fuzzy Clustering\n",
    "In fuzzy clustering, the assignment of the data points in any of the clusters is not decisive. Here, one data point can belong to more than one cluster. It provides the outcome as the probability of the data point belonging to each of the clusters. One of the algorithms used in fuzzy clustering is Fuzzy c-means clustering.\n",
    "\n",
    "This algorithm is similar in approach to the K-Means clustering. It differs in the parameters involved in the computation,  like fuzzifier and membership values. In this type of clustering method, each data point can belong to more than one cluster.  This clustering technique allocates membership values to each image point correlated to each cluster center based on the distance between the cluster center and the image point.\n",
    "\n",
    "Partitioning Clustering\n",
    "This method is one of the most popular choices for analysts to create clusters. In partitioning clustering, the clusters are partitioned based upon the characteristics of the data points. We need to specify the number of clusters to be created for this clustering method. These clustering algorithms follow an iterative process to reassign the data points between clusters based upon the distance. \n",
    "\n",
    "o   K-Means Clustering: – K-Means clustering is one of the most widely used algorithms. It partitions the data points into k clusters based upon the distance metric used for the clustering. The value of ‘k’ is to be defined by the user. The distance is calculated between the data points and the centroids of the clusters.\n",
    "\n",
    "K-means clustering is a type of unsupervised learning used when you have unlabeled data (i.e., data without defined categories or groups). This algorithm aims to find groups in the data, with the number of groups represented by the variable K. In this clustering method, the number of clusters found from the data is denoted by the letter ‘K.’\n",
    "\n",
    "PAM (Partitioning Around Medoids)\n",
    " This algorithm is also called as k-medoid algorithm. It is also similar in process to the K-means clustering algorithm with the difference being in the assignment of the center of the cluster. In PAM, the medoid of the cluster has to be an input data point while this is not true for K-means clustering as the average of all the data points in a cluster may not belong to an input data point.\n",
    "\n",
    "o   CLARA (Clustering Large Applications): – CLARA is an extension to the PAM algorithm where the computation time has been reduced to make it perform better for large data sets.\n",
    "\n",
    "It arbitrarily selects a portion of data from the whole data set, as a representative of the actual data. It applies the PAM algorithm to multiple samples of the data and chooses the best clusters from a number of iterations. It uses only random samples of the input data (instead of the entire dataset) and computes the best medoids in those samples. It works better than K-Medoids for crowded datasets. It is intended to reduce the computation time in the case of a large data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f8c55",
   "metadata": {},
   "source": [
    "3. Explain how the k-means algorithm determines the consistency of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bde0e2",
   "metadata": {},
   "source": [
    "The k-means algorithm uses an iterative approach to find the optimal cluster assignments by minimizing the sum of squared distances between data points and their assigned cluster centroid. So far, we have understood what clustering is and the different properties of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d368c47",
   "metadata": {},
   "source": [
    "4. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305942f",
   "metadata": {},
   "source": [
    "The main difference between K-Means and K-Medoids is that K-Means will form clusters based on the distance of observations to each centroid, while K-Medoid forms clusters based on the distance to medoids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449844c1",
   "metadata": {},
   "source": [
    "5. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc78b86",
   "metadata": {},
   "source": [
    "A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering calculation. The result of a clustering is presented either as the distance or the similarity between the clustered rows or columns depending on the selected distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a2995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
