{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152cdd1b",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7a194",
   "metadata": {},
   "source": [
    "Feature engineering is a machine learning technique that leverages data to create new variables that aren't in the training set. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy.Feature engineering is the process of selecting, manipulating and transforming raw data into features that can be used in supervised learning. It consists of five processes: feature creation, transformations, feature extraction, exploratory data analysis and benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba02f75",
   "metadata": {},
   "source": [
    "2. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a274fa6",
   "metadata": {},
   "source": [
    "High-dimensional data are defined as data in which the number of features (variables observed), p, are close to or larger than the number of observations (or data points), n. The opposite is low-dimensional data in which the number of observations, n, far outnumbers the number of features, p.The best way to go higher than three dimensions is to use plot facets, color, shapes, sizes, depth and so on. You can also use time as a dimension by making an animated plot for other attributes over time (considering time is a dimension in the data).High dimensional data is common in healthcare datasets where the number of features for a given individual can be massive (i.e. blood pressure, resting heart rate, immune system status, surgery history, height, weight, existing conditions, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21fd81c",
   "metadata": {},
   "source": [
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35163ec",
   "metadata": {},
   "source": [
    "The simple matching coefficient (SMC) or Rand similarity coefficient is a statistic used for comparing the similarity and diversity of sample sets.The Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It's a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667058",
   "metadata": {},
   "source": [
    "4. Sequential backward exclusion vs. sequential forward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441ff3b",
   "metadata": {},
   "source": [
    "The least number of feature selections obtained from the SFwS-Logistic Regression algorithm is one feature with an accuracy score of 0.8083. The Sequential Backward Selection algorithm generally has a longer running time than the Sequential Forward Selection.In forward selection, the model starts with no predictors and successively enters significant predictors until reaching a statistical stopping criteria. In backward elimination, the model starts with all possible predictors and successively removes non-significant predictors until reaching the stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475e0c7",
   "metadata": {},
   "source": [
    "5. Explain PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f891b3b",
   "metadata": {},
   "source": [
    "Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data points much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "So, to sum up, the idea of PCA is simple â€” reduce the number of variables of a data set, while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21e5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
