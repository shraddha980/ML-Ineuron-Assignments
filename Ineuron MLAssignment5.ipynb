{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9056ddf",
   "metadata": {},
   "source": [
    "1. What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0721d4f",
   "metadata": {},
   "source": [
    "Following are the key machine learning tasks briefed later in this article:\n",
    "\n",
    "Regression\n",
    "Classification\n",
    "Clustering\n",
    "Transcription\n",
    "Machine translation\n",
    "Anomaly detection\n",
    "Synthesis & sampling\n",
    "Estimation of probability density and probability mass function\n",
    "Similarity matching\n",
    "Co-occurrence grouping\n",
    "Causal modeling\n",
    "Link profiling\n",
    "\n",
    "Data preprocessing is the concept of changing the raw data into a clean data set. The dataset is preprocessed in order to check missing values, noisy data, and other inconsistencies before executing it to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c39c78",
   "metadata": {},
   "source": [
    "4. What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40f4d3",
   "metadata": {},
   "source": [
    "1. Inadequate Training Data\n",
    "The major issue that comes while using machine learning algorithms is the lack of quality as well as quantity of data. Although data plays a vital role in the processing of machine learning algorithms, many data scientists claim that inadequate data, noisy data, and unclean data are extremely exhausting the machine learning algorithms. For example, a simple task requires thousands of sample data, and an advanced task such as speech or image recognition needs millions of sample data examples. Further, data quality is also important for the algorithms to work ideally, but the absence of data quality is also found in Machine Learning applications. Data quality can be affected by some factors as follows:\n",
    "\n",
    "Noisy Data- It is responsible for an inaccurate prediction that affects the decision as well as accuracy in classification tasks.\n",
    "Incorrect data- It is also responsible for faulty programming and results obtained in machine learning models. Hence, incorrect data may affect the accuracy of the results also.\n",
    "Generalizing of output data- Sometimes, it is also found that generalizing output data becomes complex, which results in comparatively poor future actions.\n",
    "2. Poor quality of data\n",
    "As we have discussed above, data plays a significant role in machine learning, and it must be of good quality as well. Noisy data, incomplete data, inaccurate data, and unclean data lead to less accuracy in classification and low-quality results. Hence, data quality can also be considered as a major common problem while processing machine learning algorithms.\n",
    "\n",
    "3. Non-representative training data\n",
    "To make sure our training model is generalized well or not, we have to ensure that sample training data must be representative of new cases that we need to generalize. The training data must cover all cases that are already occurred as well as occurring.\n",
    "\n",
    "Further, if we are using non-representative training data in the model, it results in less accurate predictions. A machine learning model is said to be ideal if it predicts well for generalized cases and provides accurate decisions. If there is less training data, then there will be a sampling noise in the model, called the non-representative training set. It won't be accurate in predictions. To overcome this, it will be biased against one class or a group.\n",
    "\n",
    "Hence, we should use representative data in training to protect against being biased and make accurate predictions without any drift.\n",
    "\n",
    "4. Overfitting and Underfitting\n",
    "Overfitting:\n",
    "\n",
    "Overfitting is one of the most common issues faced by Machine Learning engineers and data scientists. Whenever a machine learning model is trained with a huge amount of data, it starts capturing noise and inaccurate data into the training data set. It negatively affects the performance of the model. Let's understand with a simple example where we have a few training data sets such as 1000 mangoes, 1000 apples, 1000 bananas, and 5000 papayas. Then there is a considerable probability of identification of an apple as papaya because we have a massive amount of biased data in the training data set; hence prediction got negatively affected. The main reason behind overfitting is using non-linear methods used in machine learning algorithms as they build non-realistic data models. We can overcome overfitting by using linear and parametric algorithms in the machine learning models.\n",
    "\n",
    "Methods to reduce overfitting:\n",
    "\n",
    "Increase training data in a dataset.\n",
    "Reduce model complexity by simplifying the model by selecting one with fewer parameters\n",
    "Ridge Regularization and Lasso Regularization\n",
    "Early stopping during the training phase\n",
    "Reduce the noise\n",
    "Reduce the number of attributes in training data.\n",
    "Constraining the model.\n",
    "Underfitting:\n",
    "\n",
    "Underfitting is just the opposite of overfitting. Whenever a machine learning model is trained with fewer amounts of data, and as a result, it provides incomplete and inaccurate data and destroys the accuracy of the machine learning model.\n",
    "\n",
    "Underfitting occurs when our model is too simple to understand the base structure of the data, just like an undersized pant. This generally happens when we have limited data into the data set, and we try to build a linear model with non-linear data. In such scenarios, the complexity of the model destroys, and rules of the machine learning model become too easy to be applied on this data set, and the model starts doing wrong predictions as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999e8a1",
   "metadata": {},
   "source": [
    "5. Demonstrate various approaches to categorical data exploration with appropriate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5bcee4",
   "metadata": {},
   "source": [
    "What are the techniques for categorical data analysis?\n",
    "General tests\n",
    "Bowker's test of symmetry.\n",
    "Categorical distribution, general model.\n",
    "Chi-squared test.\n",
    "Cochran–Armitage test for trend.\n",
    "Cochran–Mantel–Haenszel statistics.\n",
    "Correspondence analysis.\n",
    "Cronbach's alpha.\n",
    "Diagnostic odds ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31095a16",
   "metadata": {},
   "source": [
    "6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d661bc2",
   "metadata": {},
   "source": [
    "Any variable measured in a study can have missing values, including the exposure, the outcome, and confounders. When missing values are ignored in the analysis, only those subjects with complete records will be included in the analysis. This may lead to biased results and loss of power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07869a8",
   "metadata": {},
   "source": [
    "7. Describe the various methods for dealing with missing data values in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680678c",
   "metadata": {},
   "source": [
    "> Deleting the Missing Values.\n",
    "> Imputing the Missing Values.\n",
    "> Imputing the Missing Values for Categorical Features.\n",
    "> Imputing the Missing Values using Sci-kit Learn Library.\n",
    "> Using “Missingness” as a Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6466e",
   "metadata": {},
   "source": [
    "8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44083a48",
   "metadata": {},
   "source": [
    "The steps involved in data preprocessing are: Data collection, Data cleaning, Data integration, Data transformation, Data reduction, Data discretization, Data normalization or Data standardization, Feature selection, and Data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dc69a",
   "metadata": {},
   "source": [
    "9.i. What is the IQR? What criteria are used to assess it?\n",
    "ii. Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker inlength? How can box plots be used to identify outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c815b1",
   "metadata": {},
   "source": [
    "Order the data from least to greatest.\n",
    "Find the median.\n",
    "Calculate the median of both the lower and upper half of the data.\n",
    "The IQR is the difference between the upper and lower medians.\n",
    "\n",
    "A box and whisker plot—also called a box plot—displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
