{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483223f8",
   "metadata": {},
   "source": [
    "1. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d2560",
   "metadata": {},
   "source": [
    "Single linkage: computes the minimum distance between clusters before merging them. Complete linkage: computes the maximum distance between clusters before merging them. Average linkage: computes the average distance between clusters before merging them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b964a2ca",
   "metadata": {},
   "source": [
    "2. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52794462",
   "metadata": {},
   "source": [
    "Apriori Algorithm is a widely-used and well-known Association Rule algorithm and is a popular algorithm used in market basket analysis. It is also considered accurate and overtop AIS and SETM algorithms. It helps to find frequent itemsets in transactions and identifies association rules between these items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b82a0",
   "metadata": {},
   "source": [
    "3. In hierarchical clustering, how is the distance between clusters measured? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aec6b6",
   "metadata": {},
   "source": [
    "In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000135c",
   "metadata": {},
   "source": [
    "4. Discuss the k-means algorithm's advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a8b0a",
   "metadata": {},
   "source": [
    "Advantages of k-means\n",
    "Relatively simple to implement.\n",
    "\n",
    "Scales to large data sets.\n",
    "\n",
    "Guarantees convergence.\n",
    "\n",
    "Can warm-start the positions of centroids.\n",
    "\n",
    "Easily adapts to new examples.\n",
    "\n",
    "Generalizes to clusters of different shapes and sizes, such as elliptical clusters.\n",
    "\n",
    "Disadvantages of k-means\n",
    "Choosing \n",
    " manually.\n",
    "\n",
    "Use the “Loss vs. Clusters” plot to find the optimal (k), as discussed in Interpret Results.\n",
    "\n",
    "Being dependent on initial values.\n",
    "\n",
    "For a low \n",
    ", you can mitigate this dependence by running k-means several times with different initial values and picking the best result. As \n",
    " increases, you need advanced versions of k-means to pick better values of the initial centroids (called k-means seeding). For a full discussion of k- means seeding see, A Comparative Study of Efficient Initialization Methods for the K-Means Clustering Algorithm by M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela.\n",
    "\n",
    "Clustering data of varying sizes and density.\n",
    "\n",
    "k-means has trouble clustering data where clusters are of varying sizes and density. To cluster such data, you need to generalize k-means as described in the Advantages section.\n",
    "\n",
    "Clustering outliers.\n",
    "\n",
    "Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored. Consider removing or clipping outliers before clustering.\n",
    "\n",
    "Scaling with number of dimensions.\n",
    "\n",
    "As the number of dimensions increases, a distance-based similarity measure converges to a constant value between any given examples. Reduce dimensionality either by using PCA on the feature data, or by using “spectral clustering” to modify the clustering algorithm as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e8d66",
   "metadata": {},
   "source": [
    "5. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b51ea",
   "metadata": {},
   "source": [
    "The key idea behind SVMs is to transform the input data into a higher-dimensional feature space. This transformation makes it easier to find a linear separation or to more effectively classify the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33837977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
