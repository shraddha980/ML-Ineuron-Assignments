{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c72d8a",
   "metadata": {},
   "source": [
    "1. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fc354",
   "metadata": {},
   "source": [
    "Classification problems are one of the most used categories of problem statements in Machine Learning and Data Science. When we explore the real-life application where Machine Learning and Data Science is being used by the tech giants like Google, Apple, Tesla, Microsoft, Facebook, etc., we will find that 9 out of 10 problem statements are classification problem statements. Because of its popularity, many new methods are coming every day and challenge the previously existing methods.\n",
    "\n",
    "BUT ON WHAT BASIS?\n",
    "\n",
    "The answer is simple, on the accuracy grounds. Research papers also publish the work and compare their newer approaches’ results with the already benchmarked research papers. So whenever we say that we have built a machine learning model, the first question that comes to us is, “What is the accuracy of your model?”\n",
    "\n",
    "To test our classification model, there are many methods based on which we can say that our machine learning model is better. In addition, there are some of the task specialized metrics to evaluate the model as well. But in this article, we will talk about the most popular methods used to check performance and are widely used in the machine learning and data science industries.\n",
    "\n",
    "Popular methods that are covered in this article are:\n",
    "\n",
    "Accuracy and its limitations\n",
    "Confusion Matrix\n",
    "Precision & Recall\n",
    "F1-Score\n",
    "Specificity\n",
    "Receiver Operating Characteristic Curve (ROC)\n",
    "Area Under Curve (AUC)\n",
    "Let’s start understanding every method in detail.\n",
    "\n",
    "Accuracy\n",
    "Accuracy for a classification problem is a straightforward calculation and is widely used in industries. In mathematical representation,\n",
    "\n",
    "Accuracy = ((Number of correct predictions)/(Number of total predictions))*100\n",
    "\n",
    "As we know, in classification problems, our machine learning model categorizes the input variables into different classes. We calculate the total number of classifications our model made and how many classifications are correct.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#Y_pred is the predicted target variable and \n",
    "#Y_act is the true target variable. Then,\n",
    "print(\"Accuracy = \", accuracy_score(Y_act, Y_pred))\n",
    "Limitations of Accuracy\n",
    "Although it is a very widely used metric, it has some serious limitations. Suppose you have trained a model to classify the images into two classes, “Cat” or “No-Cat”. You tested your model on 100 images containing cats, and your model gave the value corresponding to the “Cat” class all the time. So accuracy from the above formulae, ((100)/(100)*100) = 100%. Wow!!!\n",
    "\n",
    "But what if, your model is always predicting the “Cat” class only?\n",
    "\n",
    "Here is the catch. If you had tested your model over 100 images having no cats, then accuracy would be ((0)/(100)*100) = 0%, but you already have stated everywhere that your model has achieved 100% accuracy. So we must not judge the model by just accuracy metric as it works well with a balanced dataset but not with the unbalanced one.\n",
    "\n",
    "Confusion Matrix\n",
    "One of the best evaluation metrics that can be considered as the base for other performance measurements. It is called a confusion matrix as it confuses the users very often.\n",
    "\n",
    "\n",
    "Let’s take an example to learn the four terms, True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN), which constitute the confusion matrix. All the performance measures will be defined using these terms.\n",
    "\n",
    "Suppose we want to eat an apple, and as we are health conscious, we built a machine learning classification model that takes the apple's image as input and predicts whether it is fresh or old. Luckily, we have our own farm, and we know which apple belongs to the fresh category and which apples don’t. Based on this, let’s define four important terms.\n",
    "\n",
    "True Positive (TP): This is when our model says an apple to be fresh and that apple was actually fresh.\n",
    "\n",
    "True Negative (TN): This is when our model says an apple to be old and that apple was actually old.\n",
    "\n",
    "False Positive (FP): This is when our machine learning model says an apple to be fresh, but in reality, that apple was old. This case is popularly known as Type I Error.\n",
    "\n",
    "False Negative (FN): This is when our machine learning model says an apple to be old but in reality, that apple was fresh. This case is popularly known as Type II Error.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [\"fresh\", \"old\", \"old\", \"fresh\", \"fresh\", \"fresh\"]\n",
    "y_pred = [\"fresh\", \"old\", \"fresh\", \"old\", \"fresh\", \"old\"]\n",
    "arr = confusion_matrix(y_true, y_pred, labels=[\"fresh\", \"old\"])\n",
    "print(\"Confusion Matrix = \",arr)\n",
    "tn, fp, fn, tp = arr.ravel()\n",
    "Now, if we have to re-define the accuracy term, then,\n",
    "\n",
    "\n",
    "which states, Out of all the apples we have, how many were correctly predicted as fresh and old.\n",
    "\n",
    "Precision & Recall\n",
    "Now, suppose there is a scenario where we have 100 apples. We predicted these apple types using our two different classification models; they segregated the apples as “fresh” and “old”. When we observed the predictions, we found,\n",
    "\n",
    "Model 1 : TP = 68, FN = 22, FP = 0, TN = 10.\n",
    "\n",
    "Model 2 : TP = 90, FN = 0, FP = 4, TN = 6.\n",
    "\n",
    "Now, suppose it's our farm, and we want to store the apple. But the problem is if we store old apples and fresh ones together, it will convert “fresh” into “old” ones. In such a scenario, we must be penalizing False Positives because, in such a scenario, it will convert all the fresh ones into old ones. Precision is a measure for that only. In the above case, Model 1 is preferred.\n",
    "\n",
    "\n",
    "Now take another scenario, we want to sell the apple as soon as possible to make extra profit. In such a scenario, there is no problem of mixing up, but the goal is to penalize the False Negatives so that more and more apples get predicted as fresh. A recall is a measure of that only. In the above case, Model 2 is preferred.\n",
    "\n",
    "\n",
    "Note: Recall is also called Sensitivity and True Positive Rate (TPR)\n",
    "\n",
    "F1-Score\n",
    "Now, suppose we have just started a supermarket, where the customers are less initially. We want a balance between the storage of apples and the sales of apples because we don’t know how many days it will take to finish all the stock. In this scenario, we need to pick the model with a higher F1-Score.\n",
    "\n",
    "\n",
    "Specificity\n",
    "Suppose we want our model to be perfectly sure about the old apples to eliminate them from our stock. We will try to penalize the False Positives and make our model surely predict the “older” ones in such a scenario. Specificity gives us a measure of that.\n",
    "\n",
    "\n",
    "ROC\n",
    "If we are familiar with the binary classification methodology, we must know that our model predicts probability. In our example earlier, suppose the model receives the image of an apple and predicts that it is “fresh” with the probability of X%. If the value of X is greater than the threshold value, then we say that model predicted the apple as “fresh”; otherwise, we say that model predicted that apple as “old”. But what if we change that threshold?\n",
    "\n",
    "We plot True Positive Rate (TPR)/Recall/Sensitivity as our Y-axis and False Positive Rate (FPR) as our X-axis for a varying threshold value. We can clearly say that Model 3 is better than Model 2, and Model 2 is better than Model 1 as we are increasing the True positive rate.\n",
    "\n",
    "\n",
    "AUC\n",
    "Suppose we built a random model to classify our apples as fresh and old. It has a 50% probability to perform the classification, and hence the line as x=y in the above image has an area under the curve as 0.5. If we have built a perfect model, then our model would classify our apples with 100% probability, and here, AUC = 1. So we can say that “More the area under the curve, the better the model”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec92b44",
   "metadata": {},
   "source": [
    "2. Is it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa689b6b",
   "metadata": {},
   "source": [
    "Choose a Robust Algorithm\n",
    "Think of machine learning algorithms as the engines of machine learning. Algorithms are responsible for transforming data sets into accurate predictions. Selecting the right algorithm for your model is essential to its performance. So, how do you know which one works best?\n",
    "\n",
    "There’s no straightforward answer. Some problems call for a highly specific approach, while others are open for interpretation. Here are a few questions to keep in mind when narrowing down your list:\n",
    "\n",
    "What kind of problem are you solving?\n",
    "What computing resources are available?\n",
    "What type of data are you processing? How much data are you processing?\n",
    "How scalable do you want your model to be?\n",
    "Which business goals would you like your model to fulfill?\n",
    "It isn’t easy to know which algorithm will work best right off the bat. We recommend starting with a list of potentials, pulling your data through, and comparing the performance of each one.\n",
    "\n",
    "2.  Use Large, High-Quality Training Datasets\n",
    "\n",
    "Have you heard of the phrase “garbage in, garbage out”? When it comes to the quality of your training data, no saying could hold more truth. If your objective is to improve the performance of ML models, it’s best to start with the information they’re fed.\n",
    "\n",
    "For example, let’s say you’re training an algorithm to flag suspicious credit card charges. For your model to accurately predict whether a charge is likely to be fraudulent, you need to train it with accurately labeled credit card transaction data.\n",
    "\n",
    "When it comes to training data for machine learning algorithms, quantity matters as much as quality. An algorithm for detecting credit card fraud that was trained using 10,000 transactions will perform better than one that was trained using 100.\n",
    "\n",
    "A machine learning model that learns using low-quality or insufficient training data will perform poorly — regardless of whatever algorithm powers it.\n",
    "\n",
    "3.  Validate and Test Your Machine Learning Model\n",
    "\n",
    "There’s only one way to check whether your model is accurate: test it. That’s where validation data comes in.\n",
    "\n",
    "While training data is used to teach your algorithm to identify patterns, testing data evaluate its accuracy. Why test your machine learning model? Testing tells you whether you chose the right algorithm and defined the right parameters by measuring your model’s accuracy, precision, and efficiency.\n",
    "\n",
    "How do you validate your machine learning model? Let’s walk through an example. Say you’re training your model to detect whether or not a vehicle is present in an image. Training data would include labeled images of vehicles. Testing data, on the other hand, would consist of unlabelled images.\n",
    "\n",
    "\n",
    "Outsource High-Quality Image Labeling for Machine Learning\n",
    "One of the best ways to improve the performance of your machine learning model is to feed it high-quality training data. But this is easier said than done.\n",
    "\n",
    "Training data for machine learning can be challenging to find, collect, and annotate. That’s why AI companies rely on professional data annotation services.\n",
    "\n",
    "Keymakr offers high-quality video and image annotation for machine learning. We have the people, processes, and tools required to boost the performance of your machine learning model.\n",
    "\n",
    "Give your machine learning algorithm the pixel-perfect training data it craves with our dedicated team of data annotators, cutting-edge tools, and quality control workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715335b0",
   "metadata": {},
   "source": [
    "3. Is it possible to use a classification model for numerical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b980d",
   "metadata": {},
   "source": [
    "Yes, classification can be done on numerical data. Numerical data can be of quantitative nature and hence can be classidfied into two or multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f442892",
   "metadata": {},
   "source": [
    "4.Make quick notes on:\n",
    "         1. The process of holding out\n",
    "         2. Cross-validation by tenfold\n",
    "         3. Adjusting the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1036e9b",
   "metadata": {},
   "source": [
    "The hold-out method for training the machine learning models is a technique that involves splitting the data into different sets: one set for training, and other sets for validation and testing. The hold-out method is used to check how well a machine learning model will perform on the new data.  In this post, you will learn about the hold-out method used during the process of training the machine learning model. \n",
    "\n",
    "With Cross-Validation data set which we divide randomly into 10 parts. We use 9 of those parts for training and reserve one tenth for testing. We repeat this procedure 10 times each time reserving a different tenth for testing\n",
    "\n",
    "What is a Parameter in a Machine Learning Model? A model parameter is a configuration variable that is internal to the model and whose value can be estimated from the given data. They are required by the model when making predictions. Their values define the skill of the model on your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d581d9d",
   "metadata": {},
   "source": [
    "5.Define the following terms: \n",
    "         1. Purity vs. Silhouette width\n",
    "         2. Boosting vs. Bagging\n",
    "         3. The eager learner vs. the lazy learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb977da",
   "metadata": {},
   "source": [
    "The main difference between the cluster purity and silhouette width is that the former ignores the intra-cluster variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf4d30",
   "metadata": {},
   "source": [
    "Bagging and boosting are different ensemble techniques that use multiple models to reduce error and optimize the model. The bagging technique combines multiple models trained on different subsets of data, whereas boosting trains the model sequentially, focusing on the error made by the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ad1c5",
   "metadata": {},
   "source": [
    "Eager learning is a type of machine learning where the system constructs a generalized model during the training phase, before any queries are made. This approach is in contrast to lazy learning, where the model is not built until a prediction is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0bb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
